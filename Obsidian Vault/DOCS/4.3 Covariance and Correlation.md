# 4.3 Covariance and Correlation
2023-04-25 | 22:18
{Subject}:[[STTN 215]]
{Section}:[[04 Expected Values]]
{Tags}: #Mathematics #Probability 
{Related}:

--- 
$E(X)=$ measure of position and $Var(X)=$ measure of spread
Suppose we now have 2-dimensional random variable $(X,Y)$

**Definition 4.4**
The `covariance` of $X$ and $Y$ is defined as
$$
\large
\begin{align}
Cov(X,Y)&=E\{(X-E(X))(Y-E(Y))\} \\
&=E\{(X-\mu_X)(Y-\mu_Y)\}
\end{align}
$$

---
1. Discrete case: $\large Cov(X,Y)=\sum_x\sum_y(x-\mu_X)(y-\mu_Y )p_{(X,Y)}(x,y)$
2. Continuous case: $Cov(X,Y)=\int^{\infty}_{-\infty}\int^{\infty}_{-\infty}(x-\mu_X)(y-\mu_Y ).f_{X,Y}(x,y)dydx$

The covariance of two random variables is a measure of their joint variation or the degree of linear association between them.

---
**Theorem 4.6**
$$
Cov(X,Y)=E(XY)-E(X)E(Y)
$$
proof:
$$
\large
\begin{align}
Cov(X,Y)&=E\{(X-\mu_X)(Y-\mu_Y)\} \\
&=E(XY-\mu_YX-\mu_XY+\mu_X\mu_Y) \\
&=E(XY)-\mu_yE(X)-\mu_XE(Y)+\mu_X\mu_Y \\
&=E(XY)-\mu_X\mu_Y-\mu_X\mu_Y+\mu_X\mu_Y \\
&=E(XY)-E(X)E(Y)
\end{align}
$$

---
**Remarks:**
1. $\large Cov(X,X)=E\{(X-E(X))(X-E(X))\}=E\{(X-E(X))^2\}=Var(X)$
2. if $X$ and $Y$ are independent ,then the following is true:
$$
\large
\begin{align}
Cov(X,Y)&=E(XY)-E(X)E(Y) \\
&=E(X)E(Y)-E(X)E(Y) \\
&=0
\end{align}
$$
but this does not mean that if $Cov(X,Y)=0$ that they are independent.

---
**Properties of $Cov(X,Y)$**
1. $Cov(a+X,Y)=Cov(X,Y)$
2. $Cov(aX,bY)=abCov(X,Y)$
3. $Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)$
4. $Cov(aW+bX,cY+dZ)=avCov(W,Y)+adCov(W,Z)+bcCov(X,Y)+bdCov(X,Z)$

---
**Variance of a linear combination of random variables**
We already know that:
$$
\large
\begin{align}
E(X_1+X_2+\dots+X_n)&=E(X_1)+\dots+E(X_n) \\
E(X_1,X_2\dots X_n)&=E(X_1).E(X_2)\dots E(X_n) \hspace{1 cm} \text{(if $X_1,\dots,X_n$ are independent)} \\
\text{now:} \\
Var(X_1+X_2+\dots+X_n)&=?
\end{align}
$$
**Theorem 4.7:**
Suppose $X_1,X_2,\dots ,X_n$ are $n$ random variables. Then:
$$
\large
Var\left(\sum^n_{k=1}X_k\right)=\sum^n_{k=1}Var(X_k)+\sum^n_{i=1}\sum^n_{j=1}Cov(X_i,X_j)\text{  with $j \neq i$}
$$
**Proof:**

---
**Definition 4.5**
Suppose $(X,Y)$ is a 2-dimensional random variable. Then we define the `correlation coefficient` between $X$ and $Y$ as:
$$
\large
\rho =\frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}
$$
where $Cov(X,Y)=E(XY)-E(X)E(Y); Var(X)=E(X^2)-[E(X)]^2; Var(Y)=E(Y^2)-[E(Y)]^2$
**Theorem 4.8:**
$$ 
-1\le\rho\le1
$$
Proof:

---
**Remark:**
The coefficient of correlation measures the degree of `linear` relation between two random variables $X$ and $Y$.
- If $\rho = 1$ there is a perfect, positive linear relationship $X$ and $Y$.
- As $\rho = -1$ there is a perfect negative linear relationship $X$ and $Y$.
- As $\rho=0$ there is no `linear` relationship between $X$ and $Y$. Note thate there might exist other types of relationships between $X$ and $Y$.

Furthermore, the coeficient of correlation is unitless, since the units in the numerator and the units in the denominator cancel.
Because $-1\le\rho\le1$, the coefficient of correlation is much easier to interpret than the covariance.

--- 
{Efundi Lecture Notes}: [Covariance and Correlation](https://efundi.nwu.ac.za/access/content/group/abd2a584-0a55-418c-9973-de94cd06741e/Slides/STTN215_Chapter4_Expected_Values.pdf)